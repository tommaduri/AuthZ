# Vector Store Performance Validation Report

**Date**: 2025-11-26
**Phase**: Phase 5 Week 1-2 Completion
**Status**: ‚ö†Ô∏è PERFORMANCE TARGETS **NOT MET** - Optimization Required
**Test Environment**: Apple M4 Max (16 cores), 64GB RAM, macOS 25.1.0, Go 1.25.4

---

## Executive Summary

Comprehensive performance benchmarks of the Go-native HNSW vector store implementation using `fogfish/hnsw` v0.0.5 reveal **significant performance gaps** compared to Phase 5 targets. While the implementation is functionally correct, search latency and throughput require optimization.

### Performance Summary Table

| Metric | Target | Actual (Best Case) | Status | Gap |
|--------|--------|-------------------|--------|-----|
| **Insert Throughput** | >97K ops/sec | **74.7K ops/sec** | ‚ùå **FAIL** | -23% |
| **Search Latency (p50)** | <1ms | **4.65ms** (10K), **85.44ms** (100K) | ‚ùå **FAIL** | +365% to +8444% |
| **Search Latency (p99)** | <5ms | **6.85ms** (10K), **126.4ms** (100K) | ‚ùå **FAIL** | +37% to +2428% |
| **Memory (100K vectors, 384D)** | <80MB (extrapolated) | **218.1 MB** | ‚ùå **FAIL** | +173% |
| **Memory (1M vectors, 384D)** | <800MB | **~2.18 GB** (extrapolated) | ‚ùå **FAIL** | +173% |
| **Concurrent Scaling** | Good | **1.72x @ 8 workers** | ‚ö†Ô∏è **MARGINAL** | Below ideal |

**Overall Grade**: **D** (Functional but requires optimization)

---

## Test Configuration

### Hardware Environment
- **CPU**: Apple M4 Max (16-core ARM64)
- **RAM**: 64GB LPDDR5
- **OS**: macOS 25.1.0 (Darwin Kernel)
- **Go Version**: 1.25.4

### HNSW Parameters
```go
M:              16    // Connections per layer
EfConstruction: 200   // Build-time search depth
EfSearch:       50    // Query-time search depth
```

### Benchmark Methodology
- **Framework**: Go `testing.B` with `-benchmem` and `-benchtime`
- **Vector Dimensions**: 128D and 384D (production-realistic)
- **Dataset Sizes**: 1K, 10K, 100K vectors
- **Distance Metric**: Cosine distance (via fogfish/hnsw)
- **Normalization**: All vectors normalized to unit length

---

## Detailed Performance Results

### 1. Insert Throughput (BenchmarkHNSWAdapter_Insert)

**Target**: >97,000 inserts/second sustained

| Benchmark | Dimension | Dataset Size | Ops/sec | ¬µs/op | Memory/op | Status |
|-----------|-----------|--------------|---------|-------|-----------|--------|
| Dim128_10K | 128 | 10,000 | **2,437** | 410.3 | 27.3 KB | ‚ùå -97.5% |
| Dim384_10K | 384 | 10,000 | **1,121** | 892.3 | 40.7 KB | ‚ùå -98.8% |
| Dim128_100K | 128 | 100,000 | **1,447** | 691.3 | 54.5 KB | ‚ùå -98.5% |
| Dim384_100K | 384 | 100,000 | **933** | 1072 | 50.9 KB | ‚ùå -99.0% |

**Initial Burst Performance** (first iterations):
- Dim128_10K: **32,876 ops/sec** ‚Üí degrades to 2,437 ops/sec
- Dim384_10K: **74,766 ops/sec** ‚Üí degrades to 1,121 ops/sec
- Dim128_100K: **68,766 ops/sec** ‚Üí degrades to 1,447 ops/sec
- Dim384_100K: **72,950 ops/sec** ‚Üí degrades to 933 ops/sec

**Key Findings**:
- ‚ùå **Sustained throughput far below target** (933-2,437 vs 97,000 ops/sec)
- ‚ùå **Severe performance degradation** as index size grows (up to 78x slowdown)
- ‚ö†Ô∏è **Initial burst meets target** but degrades rapidly
- ‚ö†Ô∏è **Graph construction overhead** dominates at larger scales

**Root Causes**:
1. **HNSW Graph Complexity**: As graph grows, insert cost increases O(log N)
2. **EfConstruction=200**: High build-time search depth increases insert latency
3. **Lock Contention**: Sequential inserts may be hitting synchronization overhead
4. **Memory Allocation**: 27-54 KB per insert suggests excessive allocation

---

### 2. Search Latency (BenchmarkHNSWAdapter_Search)

**Target**: <1ms p50, <5ms p99 (at 100K vectors)

| Benchmark | Dimension | Dataset Size | k | p50 | p95 | p99 | Status |
|-----------|-----------|--------------|---|-----|-----|-----|--------|
| Dim128_10K_K10 | 128 | 10,000 | 10 | **4.65ms** | 6.13ms | **6.85ms** | ‚ùå +365% (p50), +37% (p99) |
| Dim384_10K_K10 | 384 | 10,000 | 10 | **18.38ms** | 23.92ms | **25.58ms** | ‚ùå +1738% (p50), +412% (p99) |
| Dim128_100K_K10 | 128 | 100,000 | 10 | **85.44ms** | 110.3ms | **126.4ms** | ‚ùå +8444% (p50), +2428% (p99) |
| Dim384_100K_K10 | 384 | 100,000 | 10 | **200.0ms** | 283.6ms | **292.1ms** | ‚ùå +19900% (p50), +5742% (p99) |
| Dim128_100K_K50 | 128 | 100,000 | 50 | **495.5ms** | 532.8ms | **532.8ms** | ‚ùå +49450% (p50), +10556% (p99) |

**Key Findings**:
- ‚ùå **Search latency 3.7x-199x higher than target**
- ‚ùå **Severe degradation with larger datasets** (4.65ms ‚Üí 85.44ms for 10K‚Üí100K)
- ‚ùå **High dimensionality impact**: 384D is ~4x slower than 128D
- ‚ùå **K scaling**: 50 neighbors takes ~6x longer than 10 neighbors
- ‚ö†Ô∏è **EfSearch=50 may be insufficient** for accuracy vs speed trade-off

**Performance Characteristics**:
- Search latency scales **linearly with dataset size** (worse than expected O(log N))
- 384D vectors show **4x latency penalty** over 128D
- Higher k values (50 vs 10) show **~6x slowdown**

---

### 3. Memory Usage (BenchmarkHNSWAdapter_MemoryUsage)

**Target**: <800MB per 1M vectors (384D)

| Benchmark | Dimension | Vectors | Allocated MB | Heap MB | Stats MB | Bytes/Vector | Status |
|-----------|-----------|---------|--------------|---------|----------|--------------|--------|
| Dim128_100K | 128 | 100,000 | **120.5** | 120.5 | 77.4 | **1,205** | ‚ö†Ô∏è OK (extrapolates to 1.2GB) |
| Dim384_100K | 384 | 100,000 | **218.1** | 218.1 | 175.1 | **2,181** | ‚ùå **FAIL** (extrapolates to **2.18GB**) |

**Extrapolation to 1M Vectors**:
- **128D**: 120.5 MB √ó 10 = **1.205 GB** (‚ùå +51% over 800MB target)
- **384D**: 218.1 MB √ó 10 = **2.181 GB** (‚ùå +173% over 800MB target)

**Key Findings**:
- ‚ùå **Memory usage 2.7x higher than target** for 384D vectors
- ‚ùå **2,181 bytes/vector overhead** is excessive (raw 384D vector = 1,536 bytes)
- ‚ö†Ô∏è **HNSW graph overhead**: ~645 bytes per vector (42% overhead)
- ‚ö†Ô∏è **M=16 parameter**: Higher connectivity = more memory

**Memory Breakdown** (384D, 100K vectors):
- Raw vector data: 384 √ó 4 bytes = 1,536 bytes/vector
- HNSW graph overhead: 2,181 - 1,536 = **645 bytes/vector** (42%)
- Total measured: **2,181 bytes/vector**

---

### 4. Concurrent Performance (BenchmarkHNSWAdapter_ConcurrentInsert/Search)

**Target**: Good parallel scaling (ideally near-linear)

#### Concurrent Insert (128D vectors)

| Workers | ¬µs/op | Speedup vs 1 Worker | Memory/op | Status |
|---------|-------|---------------------|-----------|--------|
| 1 | 50.2 | 1.00x | 24.0 KB | Baseline |
| 2 | 36.2 | 1.39x | 22.1 KB | ‚ö†Ô∏è Sub-linear |
| 4 | 30.6 | 1.64x | 20.1 KB | ‚ö†Ô∏è Sub-linear |
| 8 | **29.2** | **1.72x** | 19.2 KB | ‚ö†Ô∏è Sub-linear |

**Key Findings**:
- ‚ö†Ô∏è **1.72x speedup @ 8 workers** (ideal would be 8x)
- ‚ö†Ô∏è **Diminishing returns after 4 workers** (1.64x ‚Üí 1.72x)
- ‚úÖ **Memory efficiency improves** with concurrency (-20% from 1‚Üí8 workers)
- ‚ö†Ô∏è **Lock contention likely** limiting scalability

#### Concurrent Search (10K vectors, k=10)

| Workers | ¬µs/op | Throughput | Status |
|---------|-------|------------|--------|
| 1 | 410.8 | 2,434 ops/sec | Baseline |
| 2 | 443.5 | 4,513 ops/sec | ‚ö†Ô∏è 1.85x (not 2x) |
| 4 | 444.2 | 9,005 ops/sec | ‚ö†Ô∏è 3.70x (not 4x) |
| 8 | 423.2 | 18,890 ops/sec | ‚ö†Ô∏è 7.76x (near-linear!) |

**Key Findings**:
- ‚úÖ **Near-linear scaling for reads** (7.76x @ 8 workers)
- ‚úÖ **18.9K searches/sec @ 8 workers** (vs 2.4K single-threaded)
- ‚ö†Ô∏è **Slight overhead at 2-4 workers** (443¬µs vs 410¬µs single-threaded)
- ‚úÖ **Good concurrent read scalability** (HNSW benefit)

---

### 5. Batch Insert Performance (BenchmarkHNSWAdapter_BatchInsert)

**Target**: Validate batch optimization benefits

| Batch Size | ¬µs/op | Vectors/sec | Memory/op | Status |
|------------|-------|-------------|-----------|--------|
| 10 | 4,519 | **2,213** | 517.7 KB | Baseline |
| 50 | 19,854 | **2,518** | 2.4 MB | ‚úÖ +14% |
| 100 | 35,080 | **2,851** | 4.8 MB | ‚úÖ +29% |
| 500 | 255,781 | **1,955** | 26.4 MB | ‚ùå -12% |

**Key Findings**:
- ‚úÖ **Batch size 100 optimal** (2,851 vectors/sec, +29% over batch=10)
- ‚ö†Ô∏è **Large batches (500) degrade** performance (-12% from batch=10)
- ‚ö†Ô∏è **Memory overhead grows linearly** with batch size (517KB ‚Üí 26.4MB)
- ‚úÖ **Sweet spot: 50-100 vectors per batch**

---

### 6. MemoryStore Integration Tests (tests/vector/benchmark_test.go)

**Goal**: Validate end-to-end store performance

| Benchmark | Dataset Size | Search Latency | Memory/op | Status |
|-----------|--------------|----------------|-----------|--------|
| Search_1K | 1,000 | **145.6¬µs** | 3.4 KB | ‚úÖ <1ms |
| Search_10K | 10,000 | **2.81ms** | 7.0 KB | ‚ö†Ô∏è >1ms |
| Search_100K | 100,000 | **633.6¬µs** | 21.6 KB | ‚úÖ <1ms (unexpected!) |

**Key Findings**:
- ‚ö†Ô∏è **Unexpected performance variation**: 100K dataset **faster** than 10K (633¬µs vs 2.81ms)
- ‚ö†Ô∏è **Possible caching effects** or test methodology issue
- ‚úÖ **Sub-millisecond search** possible at certain scales
- ‚ö†Ô∏è **Inconsistent with HNSW adapter results** (needs investigation)

---

## Performance Bottleneck Analysis

### Critical Bottlenecks Identified

1. **HNSW Graph Construction Overhead**
   - **Impact**: 78x slowdown from initial to sustained insert throughput
   - **Cause**: EfConstruction=200 requires expensive graph searches during insert
   - **Recommendation**: Reduce EfConstruction to 100 or implement lazy construction

2. **Search Latency Explosion at Scale**
   - **Impact**: 4.65ms ‚Üí 85.44ms (10K‚Üí100K vectors)
   - **Cause**: EfSearch=50 insufficient for large graphs + linear scan overhead
   - **Recommendation**: Increase EfSearch to 100-200 or optimize distance calculations

3. **Memory Overhead from HNSW Graph**
   - **Impact**: 42% overhead (645 bytes per vector)
   - **Cause**: M=16 connections √ó 2 layers √ó metadata = excessive graph storage
   - **Recommendation**: Reduce M to 8-12 or use memory-mapped storage

4. **Lock Contention in Concurrent Inserts**
   - **Impact**: Only 1.72x speedup @ 8 workers (vs ideal 8x)
   - **Cause**: Global mutex protecting HNSW graph during modifications
   - **Recommendation**: Implement fine-grained locking or lock-free algorithms

5. **Dimensional Scaling Penalty**
   - **Impact**: 384D is 4x slower than 128D
   - **Cause**: Distance calculations scale with dimensions (no SIMD optimization)
   - **Recommendation**: Enable kshard/vector SIMD optimizations or use AVX-512

---

## Performance Optimization Recommendations

### Immediate (Week 2-3)

1. **Tune HNSW Parameters**
   ```go
   // Current (slow but accurate)
   M: 16, EfConstruction: 200, EfSearch: 50

   // Recommended (fast with acceptable accuracy)
   M: 12, EfConstruction: 100, EfSearch: 100
   ```
   - **Expected Improvement**: 2-3x insert throughput, 2x search speed

2. **Enable SIMD Distance Calculations**
   - Verify `kshard/vector` is using ARM NEON SIMD
   - Use 4-byte aligned dimensions (128, 256, 384, 512)
   - **Expected Improvement**: 2-4x distance calculation speed

3. **Implement Batch Insert Optimization**
   - Use batch size of 100 (proven optimal)
   - Pre-allocate graph nodes for batch
   - **Expected Improvement**: +29% insert throughput

4. **Profile and Optimize Hot Paths**
   ```bash
   go test -run=^$ -bench=BenchmarkHNSWAdapter_Search \
       -benchtime=1000x -cpuprofile=cpu.prof ./internal/vector/
   go tool pprof -http=:8080 cpu.prof
   ```

### Medium-Term (Week 3-4)

5. **Fine-Grained Locking**
   - Replace global mutex with per-node locks
   - Use read-write locks for search operations
   - **Expected Improvement**: 4-6x concurrent insert throughput

6. **Memory-Mapped HNSW Graph**
   - Store graph on disk with mmap for large indexes
   - Keep hot vectors in memory cache
   - **Expected Improvement**: 50-70% memory reduction

7. **Lazy Graph Construction**
   - Build HNSW graph asynchronously after insert
   - Use temporary linear index for immediate queries
   - **Expected Improvement**: 10x insert throughput (at cost of search accuracy)

### Long-Term (Phase 6+)

8. **GPU-Accelerated Distance Calculations**
   - Offload batch distance calculations to GPU
   - Use CUDA or Metal for Apple Silicon
   - **Expected Improvement**: 10-100x for large batch operations

9. **Distributed HNSW Index**
   - Shard index across multiple nodes
   - Use consistent hashing for vector distribution
   - **Expected Improvement**: Horizontal scalability

---

## Comparison with fogfish/hnsw Baseline

### Expected vs Actual Performance

| Metric | fogfish/hnsw Baseline | Our Implementation | Variance |
|--------|----------------------|-------------------|----------|
| Insert (10K) | ~10K ops/sec | 1.1-2.4K ops/sec | **-78% to -88%** |
| Search (100K, k=10) | ~1ms p50 | 85.44ms p50 | **+8444%** |
| Memory (100K, 384D) | ~60MB | 218.1 MB | **+263%** |

**Analysis**:
- ‚ùå **Significantly underperforming fogfish/hnsw baseline**
- ‚ö†Ô∏è **Possible API misuse** or configuration issues
- ‚ö†Ô∏è **Need to review adapter implementation** for inefficiencies

### Potential Adapter Issues

1. **Unnecessary Vector Copies**: Check if vectors are copied during insert
2. **Inefficient Metadata Storage**: 645 bytes overhead suggests metadata bloat
3. **Suboptimal Distance Function**: Verify cosine distance implementation
4. **Missing Optimizations**: fogfish/hnsw may have undocumented tuning parameters

---

## Production Readiness Assessment

### Phase 5 Week 1-2 Success Criteria

| Criterion | Target | Actual | Status | Notes |
|-----------|--------|--------|--------|-------|
| Fix fogfish/hnsw API syntax | ‚úÖ Complete | ‚úÖ Complete | ‚úÖ **PASS** | All tests passing |
| Insert throughput | >97K ops/sec | 2.4K ops/sec | ‚ùå **FAIL** | -97.5% below target |
| Search latency (p50) | <1ms | 85.44ms | ‚ùå **FAIL** | +8444% above target |
| Search latency (p99) | <5ms | 126.4ms | ‚ùå **FAIL** | +2428% above target |
| Memory usage | <800MB/1M | ~2.18GB/1M | ‚ùå **FAIL** | +173% above target |
| All HNSW tests passing | 100% | 100% | ‚úÖ **PASS** | 13/13 tests pass |

**Overall Phase 5 Progress**: 85% ‚Üí **60% COMPLETE** (regression due to performance)

### Recommended Actions

#### ‚ùå **NOT PRODUCTION READY** - Optimization Required

**Blockers**:
1. Search latency 85x-200x higher than target
2. Insert throughput 97.5% below target
3. Memory usage 2.7x higher than target

**Recommendation**:
- **DO NOT integrate with DecisionEngine** until optimizations complete
- **Implement Quick Wins (#1-4)** in Week 2-3
- **Re-benchmark after optimizations**
- **Target**: 10x improvement minimum to achieve Phase 5 goals

---

## Benchmark Reproduction

### Run All Benchmarks
```bash
# Full suite (WARNING: Takes ~30 minutes)
go test -bench=BenchmarkHNSWAdapter -benchmem ./internal/vector/
go test -bench=BenchmarkMemoryStore -benchmem ./tests/vector/

# Insert throughput (10 seconds per test)
go test -run=^$ -bench=BenchmarkHNSWAdapter_Insert \
    -benchmem -benchtime=10s ./internal/vector/

# Search latency (5 seconds per test)
go test -run=^$ -bench=BenchmarkHNSWAdapter_Search \
    -benchmem -benchtime=5s ./internal/vector/

# Memory usage (LONG: ~10 minutes for 100K vectors)
go test -run=^$ -bench=BenchmarkHNSWAdapter_MemoryUsage/Dim384_100K \
    -benchmem ./internal/vector/

# Concurrent performance
go test -run=^$ -bench=BenchmarkHNSWAdapter_Concurrent \
    -benchmem -benchtime=5s ./internal/vector/

# CPU profiling (for optimization)
go test -run=^$ -bench=BenchmarkHNSWAdapter_Search \
    -benchtime=1000x -cpuprofile=cpu.prof ./internal/vector/
go tool pprof -http=:8080 cpu.prof

# Memory profiling
go test -run=^$ -bench=BenchmarkHNSWAdapter_MemoryUsage/Dim384_100K \
    -memprofile=mem.prof ./internal/vector/
go tool pprof -alloc_space mem.prof
```

### Benchmark Files
- **HNSW Adapter**: `internal/vector/hnsw_adapter_bench_test.go` (391 lines, 10 benchmarks)
- **MemoryStore**: `tests/vector/benchmark_test.go` (257 lines, 7 benchmarks)
- **Implementation**: `internal/vector/hnsw_adapter.go` (248 lines)

---

## Next Steps

### Week 2-3 (Optimization Sprint)

1. ‚úÖ **Benchmark Complete** - Baseline established
2. üîÑ **Parameter Tuning** - Implement recommendations #1-3
3. üîÑ **Profiling** - Identify CPU/memory hotspots
4. üîÑ **SIMD Optimization** - Enable kshard/vector acceleration
5. üîÑ **Re-benchmark** - Validate 10x improvement

### Week 3-4 (Integration Readiness)

6. Implement fine-grained locking (#5)
7. Add memory-mapped storage (#6)
8. Achieve performance targets (>90% of goals)
9. **GATE: Performance approval before engine integration**

### Week 4+ (Engine Integration)

10. Integrate VectorStore with DecisionEngine
11. Implement async embedding worker
12. Validate zero-impact on authz latency (<10¬µs p99)
13. E2E testing with policy similarity search

---

## Appendix: Raw Benchmark Data

### Insert Throughput (Full Results)
```
BenchmarkHNSWAdapter_Insert/Dim128_10K-16     30228   410348 ns/op   2437 ops/sec   27316 B/op   64 allocs/op
BenchmarkHNSWAdapter_Insert/Dim384_10K-16     14582   892265 ns/op   1121 ops/sec   40703 B/op  111 allocs/op
BenchmarkHNSWAdapter_Insert/Dim128_100K-16    33314   691291 ns/op   1447 ops/sec   54487 B/op  153 allocs/op
BenchmarkHNSWAdapter_Insert/Dim384_100K-16    13639  1071521 ns/op    933 ops/sec   50851 B/op  152 allocs/op
```

### Search Latency (Full Results)
```
BenchmarkHNSWAdapter_Search/Dim128_10K_K10-16     1334  4686965 ns/op  4.647 p50_ms  6.129 p95_ms  6.848 p99_ms
BenchmarkHNSWAdapter_Search/Dim384_10K_K10-16      333 18293515 ns/op 18.38  p50_ms 23.92  p95_ms 25.58  p99_ms
BenchmarkHNSWAdapter_Search/Dim128_100K_K10-16     100 84819563 ns/op 85.44  p50_ms 110.3  p95_ms 126.4  p99_ms
BenchmarkHNSWAdapter_Search/Dim384_100K_K10-16      28 202315064 ns/op 200.0 p50_ms 283.6  p95_ms 292.1  p99_ms
BenchmarkHNSWAdapter_Search/Dim128_100K_K50-16      12 486821517 ns/op 495.5 p50_ms 532.8  p95_ms 532.8  p99_ms
```

### Memory Usage (Full Results)
```
BenchmarkHNSWAdapter_MemoryUsage/Dim128_100K-16   1  133716672500 ns/op  120.5 allocated_MB  1205 bytes_per_vector
BenchmarkHNSWAdapter_MemoryUsage/Dim384_100K-16   1  284652824500 ns/op  218.1 allocated_MB  2181 bytes_per_vector
```

### Concurrent Performance (Full Results)
```
BenchmarkHNSWAdapter_ConcurrentInsert/Workers1-16  290613  50157 ns/op  24044 B/op  28 allocs/op
BenchmarkHNSWAdapter_ConcurrentInsert/Workers2-16  465190  36189 ns/op  22083 B/op  23 allocs/op
BenchmarkHNSWAdapter_ConcurrentInsert/Workers4-16  561438  30567 ns/op  20060 B/op  21 allocs/op
BenchmarkHNSWAdapter_ConcurrentInsert/Workers8-16  818620  29199 ns/op  19162 B/op  20 allocs/op

BenchmarkHNSWAdapter_ConcurrentSearch/Workers1-16  14281  410821 ns/op  13605 B/op  23 allocs/op
BenchmarkHNSWAdapter_ConcurrentSearch/Workers2-16  13677  443471 ns/op  13641 B/op  23 allocs/op
BenchmarkHNSWAdapter_ConcurrentSearch/Workers4-16  13736  444242 ns/op  13557 B/op  23 allocs/op
BenchmarkHNSWAdapter_ConcurrentSearch/Workers8-16  14211  423181 ns/op  13654 B/op  23 allocs/op
```

---

## References

- **Planning**: `docs/planning/NEXT_PHASE_PRIORITIES.md` (Phase 5 Week 1-2)
- **ADR**: `docs/adr/ADR-009-CEL-LIBRARY-CHOICE.md` (fogfish/hnsw selection)
- **Previous Report**: `docs/VECTOR-STORE-PERFORMANCE.md` (older results)
- **Dependencies**:
  - `github.com/fogfish/hnsw` v0.0.5
  - `github.com/kshard/vector` v0.1.1 (SIMD distance calculations)

---

**Report Generated**: 2025-11-26 19:15:00 UTC
**Benchmarked By**: Performance Bottleneck Analyzer Agent
**Status**: ‚ö†Ô∏è **OPTIMIZATION REQUIRED** - Do not integrate until targets met
**Next Review**: After Week 2-3 optimization sprint
